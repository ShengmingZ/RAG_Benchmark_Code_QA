retrieval_accuracy = {
    "BM25": {
        'NQ': {1: 0.2385, 3: 0.3935, 5: 0.47, 10: 0.5735, 20: 0.6605, 50: 0.7555, 100: 0.801},
        'TriviaQA': {1: 0.554, 3: 0.728, 5: 0.7865, 10: 0.865, 20: 0.914, 50: 0.9565, 100: 0.975},
        'hotpotQA': {1: 0.277, 3: 0.4225, 5: 0.47, 10: 0.534, 20: 0.5915, 50: 0.6705, 100: 0.721},
        'conala': {1: 0.0398, 3: 0.0625, 5: 0.0909, 10: 0.1364, 20: 0.1364, 50: 0.2273, 100: 0.2973},
        'pandas_numpy_eval': {1: 0.0357, 3: 0.0883, 5: 0.13, 10: 0.1538, 20: 0.2143, 50: 0.2912, 100: 0.3368},
        'DS1000': {1: 0.0372, 3: 0.0588, 5: 0.0704, 10: 0.0891, 20: 0.1283, 50: 0.1776, 100: 0.2049},
    },
    "miniLM": {
        'NQ': {1: 0.3935, 3: 0.5865, 5: 0.674, 10: 0.7665, 20: 0.8185, 50: 0.882, 100: 0.9185},
        'TriviaQA': {1: 0.503, 3: 0.707, 5: 0.7755, 10: 0.849, 20: 0.9015, 50: 0.942, 100: 0.9685},
        'hotpotQA': {1: 0.248, 3: 0.36, 5: 0.402, 10: 0.453, 20: 0.503, 50: 0.567, 100: 0.61},
        'conala': {1: 0.0341, 3: 0.0777, 5: 0.1061, 10: 0.1913, 20: 0.2775, 50: 0.4129, 100: 0.4678},
        'pandas_numpy_eval': {1: 0.0565, 3: 0.1389, 5: 0.1788, 10: 0.2527, 20: 0.3142, 50: 0.4407, 100: 0.5141},
        'DS1000': {1: 0.0499, 3: 0.0807, 5: 0.1094, 10: 0.1611, 20: 0.1993, 50: 0.2908, 100: 0.3431},
    },
    "openai-embedding": {
        'NQ': {1: 0.472, 3: 0.6635, 5: 0.7345, 10: 0.8155, 20: 0.8705, 50: 0.9225, 100: 0.9485},
        'TriviaQA': {1: 0.6275, 3: 0.8175, 5: 0.871, 10: 0.9175, 20: 0.9515, 50: 0.9785, 100: 0.989},
        'hotpotQA': {1: 0.351, 3: 0.532, 5: 0.582, 10: 0.639, 20: 0.688, 50: 0.742, 100: 0.78},
        'conala': {1: 0.004, 3: 0.0873, 5: 0.0992, 10: 0.1508, 20: 0.2113, 50: 0.3075, 100: 0.3661},
        'pandas_numpy_eval': {1: 0.0987, 3: 0.1761, 5: 0.2207, 10: 0.2924, 20: 0.3926, 50: 0.4729, 100: 0.5572},
        'DS1000': {1: 0.0434, 3: 0.0878, 5: 0.1169, 10: 0.1809, 20: 0.2275, 50: 0.3031, 100: 0.3605},
    },
    "openai-embedding_rerank_cohere": {
        'NQ': None,
        'TriviaQA': None,
        'hotpotQA': {1: 0.419, 3: 0.646, 5: 0.693, 10: 0.732, 20: 0.758, 50: 0.776, 100: 0.78},
        'conala': {1: 0.0278, 3: 0.0615, 5: 0.1091, 10: 0.1855, 20: 0.2331, 50: 0.2708, 100: 0.3631},
        'pandas_numpy_eval': {1: 0.1407, 3: 0.266, 5: 0.3179, 10: 0.3827, 20: 0.4401, 50: 0.519, 100: 0.5606},
        'DS1000': {1: 0.0522, 3: 0.0833, 5: 0.1085, 10: 0.1561, 20: 0.2127, 50: 0.2848, 100: 0.3631},
    },
    "contriever": {
        'NQ': {1: 0.189, 3: 0.3695, 5: 0.4705, 10: 0.586, 20: 0.6875, 50: 0.792, 100: 0.8495},
        'TriviaQA': {1: 0.3975, 3: 0.6155, 5: 0.714, 10: 0.799, 20: 0.881, 50: 0.94, 100: 0.969},
        'hotpotQA': {1: 0.237, 3: 0.348, 5: 0.403, 10: 0.472, 20: 0.534, 50: 0.609, 100: 0.663},
        'conala': {1: 0.017, 3: 0.0549, 5: 0.0663, 10: 0.0928, 20: 0.0928, 50: 0.161, 100: 0.215},
        'pandas_numpy_eval': {1: 0.0119, 3: 0.0268, 5: 0.0298, 10: 0.0595, 20: 0.0923, 50: 0.1949, 100: 0.255},
        'DS1000': {1: 0.0133, 3: 0.0334, 5: 0.0494, 10: 0.0545, 20: 0.0656, 50: 0.122, 100: 0.1668},
    },
    "codeT5": {
        'NQ': None,
        'TriviaQA': None,
        'hotpotQA': None,
        'conala': {1: 0.0739, 3: 0.1136, 5: 0.1402, 10: 0.1705, 20: 0.2973, 50: 0.447, 100: 0.5767},
        'pandas_numpy_eval': {1: 0.006, 3: 0.0179, 5: 0.0476, 10: 0.0709, 20: 0.1334, 50: 0.2021, 100: 0.3079},
        'DS1000': {1: 0.0064, 3: 0.0223, 5: 0.0244, 10: 0.0295, 20: 0.0433, 50: 0.0918, 100: 0.1422},
    }
}


# 'BM25': {'em': 0.6315, 'f1': 0.7166204887218547, 'prec': 0.6926273072795384, 'recall': 0.8031583333333334},
#         'miniLM': {'em': 0.0525, 'f1': 0.1984660821533895, 'prec': 0.14775189912621844, 'recall': 0.6317416666666666},
#         'openai-embedding': {'em': 0.0635, 'f1': 0.21117684822958296, 'prec': 0.16022261271110894, 'recall': 0.6570000000000006},
#         'contriever': {'em': 0.044, 'f1': 0.17035605564717382, 'prec': 0.1270591187148195, 'recall': 0.5581333333333336}

retriever_perf_llama = {
    'NQ': {
        'BM25': {'em': 0.05, 'f1': 0.1786961769736965, 'prec': 0.13503604956505902, 'recall': 0.5456583333333338},
        'miniLM': {'em': 0.646, 'f1': 0.7243253494273003, 'prec': 0.7028446331778762, 'recall': 0.8075787878787877},
        'openai-embedding': {'em': 0.6705, 'f1': 0.7523535426367305, 'prec': 0.7286757299584995, 'recall': 0.8371984848484849},
        'contriever': {'em': 0.633, 'f1': 0.7104561200428963, 'prec': 0.6893711013373296, 'recall': 0.7928833333333333}
    },
    'TriviaQA': {
        'BM25': {'em': 0.1915, 'f1': 0.3660580759880785, 'prec': 0.29741838141695204, 'recall': 0.8419083333333328},
        'miniLM': {'em': 0.1745, 'f1': 0.3471259913860253, 'prec': 0.27950000384825197, 'recall': 0.8294466450216446},
        'openai-embedding': {'em': 0.202, 'f1': 0.37776666224593414, 'prec': 0.30937274681893795, 'recall': 0.8608916666666661},
        'contriever': {'em': 0.141, 'f1': 0.32077085408830275, 'prec': 0.2508410955069186, 'recall': 0.8228712301587301}
    },
    'hotpotQA': {
        'BM25': {'em': 0.0505, 'f1': 0.174383502626325, 'prec': 0.13057835958901104, 'recall': 0.532755244755245},
        'miniLM': {'em': 0.0505, 'f1': 0.15961374764570338, 'prec': 0.12033470411281898, 'recall': 0.4730590326340328},
        'openai-embedding': {'em': 0.046, 'f1': 0.1689016592149802, 'prec': 0.12540486085884342, 'recall': 0.5360708832833833},
        'contriever': {'em': 0.034, 'f1': 0.1464155973383291, 'prec': 0.10548973484569028, 'recall': 0.4808009157509159}
    },
    'conala': {
        'BM25': {'pass@1': 0.19, 'ret_recall': 0.083, 'oracle_percent': 0.019, 'oracle_rank': 2.75, 'prompt_length': 2999.786},
        'miniLM': {'pass@1': 0.214, 'ret_recall': 0.111, 'oracle_percent': 0.029, 'oracle_rank': 2.417, 'prompt_length': 732.595},
        'openai-embedding': {'pass@1': 0.226, 'ret_recall': 0.099, 'oracle_percent': 0.024, 'oracle_rank': 2.5, 'prompt_length': 542.631},
        'codeT5': {'pass@1': 0.238, 'ret_recall': 0.147, 'oracle_percent': 0.038, 'oracle_rank': 2.125, 'prompt_length': 1232.524},
    },
    'DS1000': {
        'BM25': {'pass@1': 0.072, 'ret_recall': 0.071, 'oracle_percent': 0.029, 'oracle_rank': 2.043, 'prompt_length': 4851.369},
        'miniLM': {'pass@1': 0.06, 'ret_recall': 0.109, 'oracle_percent': 0.042, 'oracle_rank': 2.121, 'prompt_length': 2582.197},
        'openai-embedding': {'pass@1': 0.069, 'ret_recall': 0.117, 'oracle_percent': 0.054, 'oracle_rank': 2.31, 'prompt_length': 3275.248},
        'codeT5': {'pass@1': 0.102, 'ret_recall': 0.024, 'oracle_percent': 0.006, 'oracle_rank': 2.6, 'prompt_length': 4172.554},
    },
    'pandas_numpy_eval': {
        # 'BM25': ,
        # 'miniLM': ,
        # 'openai-embedding': ,
        # 'codeT5': ,
    }
}


retriever_perf_gpt = {
    'NQ': {
        'BM25': {'em': 0.254, 'f1': 0.3799936204388769, 'prec': 0.37224948402243435, 'recall': 0.47907500000000014},
        'miniLM': {'em': 0.319, 'f1': 0.45539919755019664, 'prec': 0.43953505825638856, 'recall': 0.5902750000000003},
        'openai-embedding': {'em': 0.3445, 'f1': 0.49073557800297163, 'prec': 0.4763133890449864, 'recall': 0.6221250000000003},
        'contriever': {'em': 0.267, 'f1': 0.39120396494624526, 'prec': 0.38556238871240267, 'recall': 0.4977666666666666}
    },
    'TriviaQA': {
        'BM25': {'em': 0.6315, 'f1': 0.7166204887218547, 'prec': 0.6926273072795384, 'recall': 0.8031583333333334},
        'miniLM': {'em': 0.646, 'f1': 0.7243253494273003, 'prec': 0.7028446331778762, 'recall': 0.8075787878787877},
        'openai-embedding': {'em': 0.6705, 'f1': 0.7523535426367305, 'prec': 0.7286757299584995, 'recall': 0.8371984848484849},
        'contriever': {'em': 0.633, 'f1': 0.7104561200428963, 'prec': 0.6893711013373296, 'recall': 0.7928833333333333}
    },
    'hotpotQA': {
        'BM25': {'em': 0.2905, 'f1': 0.41366158394437524, 'prec': 0.4187718714133991, 'recall': 0.46423485958485977},
        'miniLM': {'em': 0.241, 'f1': 0.3584158256474385, 'prec': 0.36329800027556275, 'recall': 0.4102590298590299},
        'openai-embedding': {'em': 0.295, 'f1': 0.42051786332163515, 'prec': 0.42574595857381387, 'recall': 0.47493446275946294},
        'contriever': {'em': 0.263, 'f1': 0.380517603212683, 'prec': 0.3865593114832381, 'recall': 0.43119434731934764}
    },
    'conala': {
        'BM25': {'pass@1': 0.357, 'ret_recall': 0.083, 'oracle_percent': 0.019, 'oracle_rank': 2.75, 'prompt_length': 2958.607},
        'miniLM': {'pass@1': 0.298, 'ret_recall': 0.111, 'oracle_percent': 0.029, 'oracle_rank': 2.417, 'prompt_length': 663.631},
        'openai-embedding': {'pass@1': 0.274, 'ret_recall': 0.099, 'oracle_percent': 0.024, 'oracle_rank': 2.5, 'prompt_length': 484.774},
        'codeT5': {'pass@1': 0.345, 'ret_recall': 0.147, 'oracle_percent': 0.038, 'oracle_rank': 2.125, 'prompt_length': 1151.321}
    },
    'DS1000': {
        'BM25': {'pass@1': 0.322, 'ret_recall': 0.071, 'oracle_percent': 0.029, 'oracle_rank': 2.043, 'prompt_length': 4806.331},
        'miniLM': {'pass@1': 0.276, 'ret_recall': 0.109, 'oracle_percent': 0.042, 'oracle_rank': 2.121, 'prompt_length': 2360.783},
        'openai-embedding': {'pass@1': 0.323, 'ret_recall': 0.117, 'oracle_percent': 0.054, 'oracle_rank': 2.31, 'prompt_length': 3139.516},
        'codeT5': {'pass@1': 0.306, 'ret_recall': 0.024, 'oracle_percent': 0.006, 'oracle_rank': 2.6, 'prompt_length': 3971.35}
    },
    'pandas_numpy_eval': {
        'BM25': {'pass@1': 0.778, 'ret_recall': 0.131, 'oracle_percent': 0.031, 'oracle_rank': 2.846, 'prompt_length': 3831.647},
        'miniLM': {'pass@1': 0.725, 'ret_recall': 0.18, 'oracle_percent': 0.046, 'oracle_rank': 2.684, 'prompt_length': 1478.94},
        'openai-embedding': {'pass@1': 0.725, 'ret_recall': 0.222, 'oracle_percent': 0.053, 'oracle_rank': 2.068, 'prompt_length': 1805.76},
        'codeT5': {'pass@1': 0.772, 'ret_recall': 0.048, 'oracle_percent': 0.012, 'oracle_rank': 3.5, 'prompt_length': 1962.892}
    }
}


code_ret_recall_llama_n_10 = {
    "conala": {
        1: {'pass@1': 0.2880952380952381, 'pass@3': 0.4255952380952382, 'pass@5': 0.4743008314436886, 'pass@10': 0.5238095238095238},
        0.8: {'pass@1': 0.2428571428571428, 'pass@3': 0.3691468253968254, 'pass@5': 0.4118008314436887, 'pass@10': 0.4523809523809524},
        0.6: {'pass@1': 0.21666666666666662, 'pass@3': 0.3387896825396826, 'pass@5': 0.39828987150415734, 'pass@10': 0.47619047619047616},
        0.4: {'pass@1': 0.1845238095238095, 'pass@3': 0.2881944444444445, 'pass@5': 0.3360733182161755, 'pass@10': 0.38095238095238093},
        0.2: {'pass@1': 0.15238095238095237, 'pass@3': 0.2441468253968254, 'pass@5': 0.2829743008314437, 'pass@10': 0.32142857142857145},
        0: {'pass@1': 0.14047619047619048, 'pass@3': 0.2147817460317461, 'pass@5': 0.2543461829176116, 'pass@10': 0.2976190476190476}
    },
    "DS1000": {
        1: {'pass@1': 0.15615917581020558, 'pass@3': 0.27813482400724965, 'pass@5': 0.34162171345619097, 'pass@10': 0.41703583093743274},
        0.8: {'pass@1': 0.1379595728451564, 'pass@3': 0.2584909045421059, 'pass@5': 0.33676267848178604, 'pass@10': 0.4604490602202273},
        0.6: {'pass@1': 0.12353796279997653, 'pass@3': 0.21912760696799594, 'pass@5': 0.2720179575347386, 'pass@10': 0.35867218212755975},
        0.4: {'pass@1': 0.14027885348823563, 'pass@3': 0.245518357976882, 'pass@5': 0.3070477875445457, 'pass@10': 0.4004366406540319},
        0.2: {'pass@1': 0.11634947876938724, 'pass@3': 0.2059071254408783, 'pass@5': 0.25494190581433146, 'pass@10': 0.3343694380879736},
        0: {'pass@1': 0.08520829666138591, 'pass@3': 0.1856552136100191, 'pass@5': 0.24744155345318583, 'pass@10': 0.343789727943046}
    },
    "pandas_numpy_eval": {
        1: {'pass@1': 0.5347305389221557, 'pass@3': 0.6707085828343313, 'pass@5': 0.7136679022906568, 'pass@10': 0.7544910179640718},
        0.8: {'pass@1': 0.5203592814371256, 'pass@3': 0.6723053892215566, 'pass@5': 0.7289468681684251, 'pass@10': 0.7844311377245509},
        0.6: {'pass@1': 0.47724550898203605, 'pass@3': 0.6163672654690615, 'pass@5': 0.672036878623705, 'pass@10': 0.7305389221556886},
        0.4: {'pass@1': 0.4574850299401197, 'pass@3': 0.6074351297405187, 'pass@5': 0.6651696606786425, 'pass@10': 0.7245508982035929},
        0.2: {'pass@1': 0.42814371257485045, 'pass@3': 0.5779441117764469, 'pass@5': 0.6364176409086588, 'pass@10': 0.7005988023952096},
        0: {'pass@1': 0.39341317365269457, 'pass@3': 0.5392215568862274, 'pass@5': 0.5979232012166144, 'pass@10': 0.6586826347305389}
    }
}


code_ret_recall_llama_n_1 = {
    "conala": {
        1: {'pass@1': 0.2976190476190476},
        0.8: {'pass@1': 0.27380952380952384},
        0.6: {'pass@1': 0.23809523809523808},
        0.4: {'pass@1': 0.19047619047619047},
        0.2: {'pass@1': 0.16666666666666666},
        0: {'pass@1': 0.14047619047619048}
    },
    "DS1000": {
        1: {'pass@1': 0.17325001466877898},
        0.8: {'pass@1': 0.1448986876332414},
        0.6: {'pass@1': 0.1550577949891451},
        0.4: {'pass@1': 0.15203602652115236},
        0.2: {'pass@1': 0.12498533122102917},
        0: {'pass@1': 0.09799477791468636}
    },
    "pandas_numpy_eval": {
        1: {'pass@1': 0.5568862275449101},
        0.8: {'pass@1': 0.5149700598802395},
        0.6: {'pass@1': 0.5029940119760479},
        0.4: {'pass@1': 0.47305389221556887},
        0.2: {'pass@1': 0.4431137724550898},
        0: {'pass@1': 0.38922155688622756}
    }
}


code_ret_recall_gpt_n_1 = {
    "conala": {
        1: {'pass@1': 0.38095238095238093},
        0.8: {'pass@1': 0.3333333333333333},
        0.6: {'pass@1': 0.32142857142857145},
        0.4: {'pass@1': 0.2857142857142857},
        0.2: {'pass@1': 0.23809523809523808},
        0: {'pass@1': 0.21428571428571427}
    },
    'DS1000': {
        1: {'pass@1': 0.3543272897963973},
        0.8: {'pass@1': 0.35302665806098305},
        0.6: {'pass@1': 0.32576228754718456},
        0.4: {'pass@1': 0.3544363277200806},
        0.2: {'pass@1': 0.29130043615169476},
        0: {'pass@1': 0.27517015783606175}
    },
    'pandas_numpy_eval': {
        1: {'pass@1': 0.7844311377245509},
        0.8: {'pass@1': 0.7724550898203593},
        0.6: {'pass@1': 0.7544910179640718},
        0.4: {'pass@1': 0.7485029940119761},
        0.2: {'pass@1': 0.6946107784431138},
        0: {'pass@1': 0.6766467065868264}
    }
}

code_ret_doc_type_llama_n_1 = {
    "conala": {
        "oracle": {'pass@1': 0.2976190476190476},
        "retrieved": {'pass@1': 0.16666666666666666},
        "distracting": {'pass@1': 0.16666666666666666},
        "random": {'pass@1': 0.21428571428571427},
        "irrelevant_dummy": {'pass@1': 0.2261904761904762},
        "irrelevant_diff": {'pass@1': 0.19047619047619047},
        "none": {'pass@1': 0.2261904761904762},
    },
    "DS1000": {
        "oracle": {'pass@1': 0.17325001466877898},
        "retrieved": {'pass@1': 0.08371569950517317},
        "distracting": {'pass@1': 0.09685892546304446},
        "random": {'pass@1': 0.09306166754679342},
        "irrelevant_dummy": {'pass@1': 0.1189765103952747},
        "irrelevant_diff": {'pass@1': 0.13781709010541962},
        "none": {'pass@1': 0.16641974222065756},
    },
    "pandas_numpy_eval": {
        "oracle": {'pass@1': 0.5568862275449101},
        "retrieved": {'pass@1': 0.38922155688622756},
        "distracting": {'pass@1': 0.38323353293413176},
        "random": {'pass@1': 0.5149700598802395},
        "irrelevant_dummy": {'pass@1': 0.5329341317365269},
        "irrelevant_diff": {'pass@1': 0.6407185628742516},
        "none": {'pass@1': 0.562874251497006},
    }
}

code_ret_doc_type_gpt_n_1 = {
    "conala": {
        "oracle": {'pass@1': 0.38095238095238093},
        "retrieved": {'pass@1': 0.21428571428571427},
        "distracting": {'pass@1': 0.2261904761904762},
        "random": {'pass@1': 0.2619047619047619},
        "irrelevant_dummy": {'pass@1': 0.30952380952380953},
        "irrelevant_diff": {'pass@1': 0.3333333333333333},
        "none": {'pass@1': 0.21428571428571427}
    },
    "DS1000": {
        "oracle": {'pass@1': 0.34041982045414537},
        "retrieved": {'pass@1': 0.25234407087954},
        "distracting": {'pass@1': 0.2598041229048094},
        "random": {'pass@1': 0.32965000293375574},
        "irrelevant_dummy": {'pass@1': 0.341829490113243},
        "irrelevant_diff": {'pass@1': 0.38344726085000685},
        "none": {'pass@1': 0.32624977996831545},
    },
    "pandas_numpy_eval": {
        "oracle": {'pass@1': 0.7904191616766467},
        "retrieved": {'pass@1': 0.6586826347305389},
        "distracting": {'pass@1': 0.6586826347305389},
        "random": {'pass@1': 0.7544910179640718},
        "irrelevant_dummy": {'pass@1': 0.7664670658682635},
        "irrelevant_diff": {'pass@1': 0.78443113772455},
        "none": {'pass@1': 0.7784431137724551}
    }
}


qa_ret_recall_llama_n_1 = {
    'NQ': {
        1: {'em': 0.2025, 'f1': 0.3984174027929825, 'prec': 0.3449570014590669, 'recall': 0.8064166666666676},
        0.8: {'em': 0.171, 'f1': 0.34678627373258397, 'prec': 0.29835325554811754, 'recall': 0.716491666666667},
        0.6: {'em': 0.1325, 'f1': 0.2896416366854257, 'prec': 0.24601839068922401, 'recall': 0.617066666666667},
        0.4: {'em': 0.097, 'f1': 0.2349356376979806, 'prec': 0.19698403946309806, 'recall': 0.5212833333333337},
        0.2: {'em': 0.0665, 'f1': 0.18463820072038975, 'prec': 0.15330608620268787, 'recall': 0.42679166666666674},
        0: {'em': 0.028, 'f1': 0.1254894433071858, 'prec': 0.09967968951158532, 'recall': 0.3234666666666664},
    },
    "TriviaQA": {
        1.0: {'em': 0.3585, 'f1': 0.5231571210883341, 'prec': 0.46082811453133665, 'recall': 0.90355},
        0.8: {'em': 0.3135, 'f1': 0.47024205445385564, 'prec': 0.4117909788471705, 'recall': 0.8362416666666664},
        0.6: {'em': 0.263, 'f1': 0.41646679689956456, 'prec': 0.3613878239903657, 'recall': 0.7710083333333331},
        0.4: {'em': 0.2175, 'f1': 0.37027932943870123, 'prec': 0.3165302015614851, 'recall': 0.7234047619047614},
        0.2: {'em': 0.1655, 'f1': 0.31764177998664433, 'prec': 0.26521479067447384, 'recall': 0.6670714285714284},
        0.0: {'em': 0.115, 'f1': 0.2638218573745877, 'prec': 0.21231379317947985, 'recall': 0.6096603174603177},
    },
    "hotpotQA": {
        1.0: {'em': 0.234, 'f1': 0.4272668402655291, 'prec': 0.3840363365819499, 'recall': 0.7897640415140422},
        0.8: {'em': 0.1965, 'f1': 0.37015850439067094, 'prec': 0.33207655388342094, 'recall': 0.694794398656899},
        0.6: {'em': 0.1565, 'f1': 0.3122402892158878, 'prec': 0.2747135038456065, 'recall': 0.6083649128649131},
        0.4: {'em': 0.1215, 'f1': 0.2515688683740639, 'prec': 0.21794441156538633, 'recall': 0.5124369005994008},
        0.2: {'em': 0.0835, 'f1': 0.19124906136555053, 'prec': 0.16291533042604434, 'recall': 0.40444761488511505},
        0.0: {'em': 0.0545, 'f1': 0.13444975467808432, 'prec': 0.1129479226129865, 'recall': 0.29293432123432134},
    }
}


qa_ret_recall_gpt_n_1 = {
    'NQ': {
        1.0: {'em': 0.5235, 'f1': 0.6806874028986782, 'prec': 0.6625575108111421, 'recall': 0.8219166666666674},
        0.8: {'em': 0.4345, 'f1': 0.5847103087798383, 'prec': 0.5726834665968233, 'recall': 0.7085083333333337},
        0.6: {'em': 0.345, 'f1': 0.4871911285523096, 'prec': 0.4798724407373933, 'recall': 0.596275},
        0.4: {'em': 0.258, 'f1': 0.39080634998766883, 'prec': 0.38963585018216573, 'recall': 0.48417499999999997},
        0.2: {'em': 0.1645, 'f1': 0.2936651897364703, 'prec': 0.2999326388248988, 'recall': 0.3707666666666666},
        0.0: {'em': 0.077, 'f1': 0.19226917154384415, 'prec': 0.1977909570216755, 'recall': 0.2618249999999998}
    },
    "TriviaQA": {
        1.0: {'em': 0.7285, 'f1': 0.8082095649605613, 'prec': 0.7828212740643429, 'recall': 0.8945416666666662},
        0.8: {'em': 0.666, 'f1': 0.7425133698113797, 'prec': 0.7203256623149807, 'recall': 0.8273124999999999},
        0.6: {'em': 0.5955, 'f1': 0.6770979084809139, 'prec': 0.6596794732385441, 'recall': 0.7574124999999997},
        0.4: {'em': 0.5325, 'f1': 0.6135144846322176, 'prec': 0.5996405003599886, 'recall': 0.6905791666666663},
        0.2: {'em': 0.4575, 'f1': 0.5441637788765754, 'prec': 0.5317435070556891, 'recall': 0.6228041666666667},
        0.0: {'em': 0.399, 'f1': 0.4880544496651674, 'prec': 0.4772081394075539, 'recall': 0.568765277777778}
    },
    "hotpotQA": {
        1.0: {'em': 0.5285, 'f1': 0.6929355057583891, 'prec': 0.7075637501738811, 'recall': 0.7497543470418472},
        0.8: {'em': 0.4525, 'f1': 0.6037805173344839, 'prec': 0.6170093566930721, 'recall': 0.6586936327561328},
        0.6: {'em': 0.3725, 'f1': 0.5137885024265315, 'prec': 0.5280138851411724, 'recall': 0.561834902597403},
        0.4: {'em': 0.3095, 'f1': 0.42815762379853006, 'prec': 0.4362275185915847, 'recall': 0.47549084804084846},
        0.2: {'em': 0.2335, 'f1': 0.33725439726073664, 'prec': 0.3450078216595463, 'recall': 0.3762200147075148},
        0.0: {'em': 0.145, 'f1': 0.2321139154753938, 'prec': 0.235179075833867, 'recall': 0.26848191946941935},
    }
}

qa_ret_doc_type_llama_n_1 = {
    'NQ': {
        "oracle": {'em': 0.2025, 'f1': 0.3984174027929825, 'prec': 0.3449570014590669, 'recall': 0.8064166666666676},
        "retrieved": {'em': 0.1095, 'f1': 0.24555840009200883, 'prec': 0.20931816465800238, 'recall': 0.5241250000000001},
        "distracting": {'em': 0.028, 'f1': 0.1254894433071858, 'prec': 0.09967968951158532, 'recall': 0.3234666666666664},
        "random": {'em': 0.0015, 'f1': 0.07286269638131553, 'prec': 0.04563178910604187, 'recall': 0.2809083333333332},
        "irrelevant_dummy": {'em': 0.005, 'f1': 0.07965801826911181, 'prec': 0.05242959518547413, 'recall': 0.267708333333333},
        "irrelevant_diff": {'em': 0.0025, 'f1': 0.10479144919838917, 'prec': 0.06518396210647913, 'recall': 0.3791999999999999},
        "none": {'em': 0.0135, 'f1': 0.12075211563717322, 'prec': 0.0783819085346971, 'recall': 0.5088416666666664},
    },
    "TriviaQA": {
        "oracle": {'em': 0.3585, 'f1': 0.5231571210883341, 'prec': 0.46082811453133665, 'recall': 0.90355},
        "retrieved": {'em': 0.298, 'f1': 0.44355579510359733, 'prec': 0.39018332836747666, 'recall': 0.7751964285714283},
        "distracting": {'em': 0.115, 'f1': 0.2638218573745877, 'prec': 0.21231379317947985, 'recall': 0.6096603174603177},
        "random": {'em': 0.0475, 'f1': 0.2229263402513595, 'prec': 0.157117585715163, 'recall': 0.66766727994228},
        "irrelevant_dummy": {'em': 0.0895, 'f1': 0.2709701209979632, 'prec': 0.20475826720507534, 'recall': 0.6895486652236649},
        "irrelevant_diff": {'em': 0.0605, 'f1': 0.23887379135147313, 'prec': 0.1703290684715535, 'recall': 0.7165988636363635},
        "none": {'em': 0.1045, 'f1': 0.27874280469851714, 'prec': 0.21019001244283403, 'recall': 0.7908571969696964}
    },
    "hotpotQA": {
        "oracle": {'em': 0.2355, 'f1': 0.4263827432332235, 'prec': 0.3846534732103997, 'recall': 0.7807540903540908},
        "retrieved": {'em': 0.1135, 'f1': 0.23571233027433397, 'prec': 0.20502697902833195, 'recall': 0.47636953046953057},
        "distracting": {'em': 0.0585, 'f1': 0.13626641146101923, 'prec': 0.11467697334487567, 'recall': 0.2907295593295593},
        "random": {'em': 0.037, 'f1': 0.12006897834196079, 'prec': 0.09308126295407551, 'recall': 0.2925680763680764},
        "irrelevant_dummy": {'em': 0.0535, 'f1': 0.13914034891210847, 'prec': 0.11293007704587482, 'recall': 0.304066187978688},
        "irrelevant_diff": {'em': 0.0355, 'f1': 0.1220129995483895, 'prec': 0.0938954764876635, 'recall': 0.3119317127317127},
        "none": {'em': 0.022, 'f1': 0.1153041117479093, 'prec': 0.08266834396783598, 'recall': 0.3603499000998999},
    }
}

qa_ret_doc_type_gpt_n_1 = {
    'NQ': {
        "oracle": {'em': 0.5195, 'f1': 0.6763192941969053, 'prec': 0.6566869380698354, 'recall': 0.8195333333333339},
        "retrieved": {'em': 0.2505, 'f1': 0.38966929494021535, 'prec': 0.3798526426123864, 'recall': 0.5012416666666667},
        "distracting": {'em': 0.077, 'f1': 0.18872411385538915, 'prec': 0.19535912540581865, 'recall': 0.25657499999999994},
        "random": {'em': 0.159, 'f1': 0.260802795891412, 'prec': 0.2606783612578137, 'recall': 0.3510083333333333},
        "irrelevant_dummy": {'em': 0.12, 'f1': 0.1983068629259111, 'prec': 0.18756693481820286, 'recall': 0.2880666666666665},
        "irrelevant_diff": {'em': 0.142, 'f1': 0.22654767431456363, 'prec': 0.22448203056732455, 'recall': 0.30703333333333327},
        "none": {'em': 0.247, 'f1': 0.40321042891286785, 'prec': 0.38063603510047195, 'recall': 0.6028666666666669},
    },
    "TriviaQA": {
        "oracle": {'em': 0.734, 'f1': 0.8119380108280341, 'prec': 0.7862939586191791, 'recall': 0.8981249999999998},
        "retrieved": {'em': 0.588, 'f1': 0.6785403596045625, 'prec': 0.6573662049086141, 'recall': 0.7640666666666663},
        "distracting": {'em': 0.4005, 'f1': 0.4888470164122673, 'prec': 0.4777201877157086, 'recall': 0.5693458333333336},
        "random": {'em': 0.65, 'f1': 0.7156878766886033, 'prec': 0.6972825684359328, 'recall': 0.7912708333333333},
        "irrelevant_dummy": {'em': 0.645, 'f1': 0.7136166898441586, 'prec': 0.6931016820557794, 'recall': 0.8066708333333333},
        "irrelevant_diff": {'em': 0.6665, 'f1': 0.727718840342482, 'prec': 0.7122035392983922, 'recall': 0.7969874999999998},
        "none": {'em': 0.7055, 'f1': 0.7732459207525455, 'prec': 0.7483291943270329, 'recall': 0.8776875},
    },
    "hotpotQA": {
        "oracle": {'em': 0.521, 'f1': 0.6883638248996307, 'prec': 0.7024575334729686, 'recall': 0.7448021645021647},
        "retrieved": {'em': 0.2615, 'f1': 0.38276433036531554, 'prec': 0.38988646984133074, 'recall': 0.43652234154734176},
        "distracting": {'em': 0.1475, 'f1': 0.23426800002167913, 'prec': 0.2381682611934461, 'recall': 0.26875334804084794},
        "random": {'em': 0.175, 'f1': 0.25957737463792346, 'prec': 0.26203237790949613, 'recall': 0.3006877705627704},
        "irrelevant_dummy": {'em': 0.1835, 'f1': 0.2633469824040573, 'prec': 0.26712892293953816, 'recall': 0.31076713564213543},
        "irrelevant_diff": {'em': 0.1955, 'f1': 0.27622148761939497, 'prec': 0.28483514853592373, 'recall': 0.3097750929625928},
        "none": {'em': 0.202, 'f1': 0.33255676607613155, 'prec': 0.3364160864602538, 'recall': 0.3910290223665226},
    }
}



# todo: top1 top5 top10 top15 top20 for code llama (16k) and gpt (16k)
code_ret_doc_selection_topk_llama_n_1 = {
    'conala': {
        'top_1': {'pass@1': 0.167, 'ret_recall': 0.004, 'oracle_percent': 0.012, 'oracle_rank': 1.0, 'prompt_length': 192.631},
        'top_5': {'pass@1': 0.226, 'ret_recall': 0.099, 'oracle_percent': 0.024, 'oracle_rank': 2.7, 'prompt_length': 542.631},
        'top_10': {'pass@1': 0.214, 'ret_recall': 0.151, 'oracle_percent': 0.019, 'oracle_rank': 6.562, 'prompt_length': 968.214},
        'top_15': {'pass@1': 0.19, 'ret_recall': 0.193, 'oracle_percent': 0.018, 'oracle_rank': 8.609, 'prompt_length': 1538.417},
        'top_20': {'pass@1': 0.143, 'ret_recall': 0.211, 'oracle_percent': 0.015, 'oracle_rank': 9.56, 'prompt_length': 2045.929}
    },
    'DS1000': {
        'top_1': {'pass@1': 0.128, 'ret_recall': 0.043, 'oracle_percent': 0.108, 'oracle_rank': 1.0, 'prompt_length': 1155.414},
        'top_5': {'pass@1': 0.069, 'ret_recall': 0.117, 'oracle_percent': 0.054, 'oracle_rank': 3.095, 'prompt_length': 3275.248},
        'top_10': {'pass@1': 0.065, 'ret_recall': 0.181, 'oracle_percent': 0.041, 'oracle_rank': 4.646, 'prompt_length': 5470.917},
        'top_15': {'pass@1': 0.105, 'ret_recall': 0.21, 'oracle_percent': 0.033, 'oracle_rank': 8.346, 'prompt_length': 6466.204},
        'top_20': {'pass@1': 0.076, 'ret_recall': 0.229, 'oracle_percent': 0.027, 'oracle_rank': 11.238, 'prompt_length': 6848.389}
    },
    'pandas_numpy_eval': {
        'top_1': {'pass@1': 0.431, 'ret_recall': 0.099, 'oracle_percent': 0.12, 'oracle_rank': 1.0, 'prompt_length': 573.844},
        'top_5': {'pass@1': 0.491, 'ret_recall': 0.222, 'oracle_percent': 0.053, 'oracle_rank': 3.182, 'prompt_length': 1935.874},
        'top_10': {'pass@1': 0.587, 'ret_recall': 0.294, 'oracle_percent': 0.036, 'oracle_rank': 4.967, 'prompt_length': 3660.982},
        'top_15': {'pass@1': 0.611, 'ret_recall': 0.357, 'oracle_percent': 0.029, 'oracle_rank': 8.736, 'prompt_length': 5122.91},
        'top_20': {'pass@1': 0.581, 'ret_recall': 0.395, 'oracle_percent': 0.024, 'oracle_rank': 10.425, 'prompt_length': 6155.419}
    }
}

# code_ret_doc_selection_gpt_n_1 = {
#     'conala': {
#         "top_1": {'pass@1': 0.17857142857142858},
#         "top_3": {'pass@1': 0.32142857142857145},
#         "top_5": {'pass@1': 0.27380952380952384},
#         "top_7": {'pass@1': 0.2619047619047619},
#         "top_9": {'pass@1': 0.27380952380952384},
#     },
#     'DS1000': {
#         "top_1": {'pass@1': 0.2352011578556201},
#         "top_3": {'pass@1': 0.3392282266424143},
#         "top_5": {'pass@1': 0.3233258033601283},
#         "top_7": {'pass@1': 0.3295707915273133},
#         "top_9": {'pass@1': 0.3460428523929668},
#     },
#     'pandas_numpy_eval': {
#         "top_1": {'pass@1': 0.6287425149700598},
#         "top_3": {'pass@1': 0.688622754491018},
#         "top_5": {'pass@1': 0.7245508982035929},
#         "top_7": {'pass@1': 0.7305389221556886},
#         "top_9": {'pass@1': 0.7305389221556886},
#     }
# }


code_ret_doc_selection_topk_gpt_n_1 = {
    'conala': {
        'top_1': {'pass@1': 0.179, 'ret_recall': 0.004, 'oracle_percent': 0.012, 'oracle_rank': 1.0, 'prompt_length': 157.976},
        'top_5': {'pass@1': 0.274, 'ret_recall': 0.099, 'oracle_percent': 0.024, 'oracle_rank': 2.5, 'prompt_length': 484.774},
        'top_10': {'pass@1': 0.286, 'ret_recall': 0.151, 'oracle_percent': 0.019, 'oracle_rank': 4.5, 'prompt_length': 875.19},
        'top_15': {'pass@1': 0.25, 'ret_recall': 0.193, 'oracle_percent': 0.018, 'oracle_rank': 6.87, 'prompt_length': 1405.833},
        'top_20': {'pass@1': 0.25, 'ret_recall': 0.211, 'oracle_percent': 0.015, 'oracle_rank': 7.68, 'prompt_length': 1897.905}
    },
    'DS1000': {
        'top_1': {'pass@1': 0.235, 'ret_recall': 0.043, 'oracle_percent': 0.108, 'oracle_rank': 1.0, 'prompt_length': 1045.637},
        'top_5': {'pass@1': 0.323, 'ret_recall': 0.117, 'oracle_percent': 0.054, 'oracle_rank': 2.31, 'prompt_length': 3139.516},
        'top_10': {'pass@1': 0.328, 'ret_recall': 0.181, 'oracle_percent': 0.041, 'oracle_rank': 4.262, 'prompt_length': 5508.363},
        'top_15': {'pass@1': 0.29, 'ret_recall': 0.21, 'oracle_percent': 0.033, 'oracle_rank': 5.692, 'prompt_length': 7801.21},
        'top_20': None
    },
    'pandas_numpy_eval': {
        'top_1': {'pass@1': 0.629, 'ret_recall': 0.099, 'oracle_percent': 0.12, 'oracle_rank': 1.0, 'prompt_length': 507.605},
        'top_5': {'pass@1': 0.725, 'ret_recall': 0.222, 'oracle_percent': 0.053, 'oracle_rank': 2.068, 'prompt_length': 1805.76},
        'top_10': {'pass@1': 0.737, 'ret_recall': 0.294, 'oracle_percent': 0.036, 'oracle_rank': 3.533, 'prompt_length': 3438.928},
        'top_15': {'pass@1': 0.701, 'ret_recall': 0.357, 'oracle_percent': 0.029, 'oracle_rank': 5.0, 'prompt_length': 4953.91},
        'top_20': {'pass@1': 0.713, 'ret_recall': 0.395, 'oracle_percent': 0.024, 'oracle_rank': 6.213, 'prompt_length': 6499.341}
    }
}


qa_ret_doc_selection_topk_llama_n_1 = {
    'NQ': {
        "top_1": {'em': 0.1095, 'f1': 0.24555840009200883, 'prec': 0.20931816465800238, 'recall': 0.5241250000000001},
        "top_5": {'em': 0.0395, 'f1': 0.18964001843018724, 'prec': 0.13606433483949573, 'recall': 0.6398166666666669},
        "top_10": {'em': 0.0635, 'f1': 0.21117684822958296, 'prec': 0.16022261271110894, 'recall': 0.6570000000000006},
        "top_15": {'em': 0.0615, 'f1': 0.2092792376873743, 'prec': 0.15926252908361213, 'recall': 0.6408500000000005},
        "top_20": {'em': 0.011, 'f1': 0.10405413490629176, 'prec': 0.06826030543636984, 'recall': 0.567},
    },
    "TriviaQA": {
        "top_1": {'em': 0.298, 'f1': 0.44355579510359733, 'prec': 0.39018332836747666, 'recall': 0.7751964285714283},
        "top_5": {'em': 0.1465, 'f1': 0.33053684436037484, 'prec': 0.2587598409651884, 'recall': 0.8381428571428564},
        "top_10": {'em': 0.202, 'f1': 0.37776666224593414, 'prec': 0.30937274681893795, 'recall': 0.8608916666666661},
        "top_15": {'em': 0.1265, 'f1': 0.3139879083299517, 'prec': 0.24103085550155745, 'recall': 0.8396643217893215},
        "top_20": {'em': 0.014, 'f1': 0.14185992170043626, 'prec': 0.08941526766481842, 'recall': 0.742952200577201}
    },
    "hotpotQA": {
        "top_1": {'em': 0.123, 'f1': 0.2421851078237043, 'prec': 0.21612439158773367, 'recall': 0.4573420121545124},
        "top_5": {'em': 0.077, 'f1': 0.1993004991696227, 'prec': 0.15884577527592247, 'recall': 0.5213618284493285},
        "top_10": {'em': 0.046, 'f1': 0.1689016592149802, 'prec': 0.12540486085884342, 'recall': 0.5360708832833833},
        "top_15": {'em': 0.0365, 'f1': 0.16167853995214196, 'prec': 0.11970466760518572, 'recall': 0.5233392094017092},
        "top_20": {'em': 0.03, 'f1': 0.1454456667893286, 'prec': 0.1045334048865944, 'recall': 0.4981687368187364}
    }
}

# qa_ret_doc_selection_gpt_n_1 = {
#     'NQ': {
#         "top_1": {'em': 0.25, 'f1': 0.3922751526271766, 'prec': 0.38121272333562434, 'recall': 0.5068083333333334},
#         "top_5": {'em': 0.3265, 'f1': 0.4678190562927842, 'prec': 0.4536730708690414, 'recall': 0.6030250000000003},
#         "top_10": {'em': 0.3445, 'f1': 0.49073557800297163, 'prec': 0.4763133890449864, 'recall': 0.6221250000000003},
#         "top_15": {'em': 0.345, 'f1': 0.49211529865522696, 'prec': 0.47809000351015996, 'recall': 0.6243750000000001},
#         "top_20": {'em': 0.349, 'f1': 0.4971804164263661, 'prec': 0.4826739429489419, 'recall': 0.6316583333333337},
#         "top_25": {'em': 0.3465, 'f1': 0.4960016340178994, 'prec': 0.48273941627714634, 'recall': 0.6302333333333335},
#         "top_30": {'em': 0.349, 'f1': 0.4989983533747123, 'prec': 0.4866316346933494, 'recall': 0.6319000000000004},
#     },
#     "TriviaQA": {
#         "top_1": {'em': 0.5915, 'f1': 0.6793697892492293, 'prec': 0.6579298292768786, 'recall': 0.7669583333333331},
#         "top_5": {'em': 0.645, 'f1': 0.7274278104337638, 'prec': 0.7058988717589391, 'recall': 0.8117416666666667},
#         "top_10": {'em': 0.6705, 'f1': 0.7523535426367305, 'prec': 0.7286757299584995, 'recall': 0.8371984848484849},
#         "top_15": {'em': 0.673, 'f1': 0.7568096785435148, 'prec': 0.7334095129282513, 'recall': 0.8432704545454547},
#         "top_20": {'em': 0.677, 'f1': 0.7580148586678289, 'prec': 0.7353244827862068, 'recall': 0.8397083333333334},
#         "top_25": {'em': 0.692, 'f1': 0.773757922403883, 'prec': 0.7502785240860561, 'recall': 0.8545772727272728},
#         "top_30": {'em': 0.684, 'f1': 0.7655187413543476, 'prec': 0.7423013409859317, 'recall': 0.8463787878787878},
#     },
#     "hotpotQA": {
#         "top_1": {'em': 0.234, 'f1': 0.3553180842318932, 'prec': 0.36114732204224864, 'recall': 0.40814816572316587},
#         "top_5": {'em': 0.2785, 'f1': 0.408006665352049, 'prec': 0.41574636258581127, 'recall': 0.46214478021978056},
#         "top_10": {'em': 0.295, 'f1': 0.42051786332163515, 'prec': 0.42574595857381387, 'recall': 0.47493446275946294},
#         "top_15": {'em': 0.305, 'f1': 0.4328039911798426, 'prec': 0.4391074937975743, 'recall': 0.4849971250971255},
#         "top_20": {'em': 0.3055, 'f1': 0.4314498604767209, 'prec': 0.4389742696871426, 'recall': 0.48317771117771147},
#         "top_25": {'em': 0.3045, 'f1': 0.4363582397053665, 'prec': 0.44220459519321953, 'recall': 0.4943199730824735},
#         "top_30": {'em': 0.314, 'f1': 0.44482039894541053, 'prec': 0.4523217160423505, 'recall': 0.4990813603063606},
#     }
# }

qa_ret_doc_selection_topk_gpt_n_1 = {
    'NQ': {
        'top_1': {'em': 0.254, 'f1': 0.391, 'prec': 0.381, 'recall': 0.5, 'ret_recall': 0.472, 'oracle_percentage': 0.472, 'oracle_rank': 1.0, 'prompt_length': 226.675},
        "top_5": {'em': 0.327, 'f1': 0.468, 'prec': 0.454, 'recall': 0.603},
        "top_10": {'em': 0.345, 'f1': 0.491, 'prec': 0.476, 'recall': 0.622},
        "top_15": {'em': 0.345, 'f1': 0.492, 'prec': 0.478, 'recall': 0.624},
        'top_20': {'em': 0.35, 'f1': 0.497, 'prec': 0.483, 'recall': 0.629, 'ret_recall': 0.871, 'oracle_percentage': 0.19, 'oracle_rank': 3.07, 'prompt_length': 2833},
        'top_40': {'em': 0.346, 'f1': 0.496, 'prec': 0.482, 'recall': 0.635, 'ret_recall': 0.911, 'oracle_percent': 0.147, 'oracle_rank': 4.238, 'prompt_length': 5577.404},
        'top_60': {'em': 0.344, 'f1': 0.492, 'prec': 0.476, 'recall': 0.638, 'ret_recall': 0.932, 'oracle_percent': 0.127, 'oracle_rank': 5.244, 'prompt_length': 8323.228},
        'top_80': {'em': 0.337, 'f1': 0.487, 'prec': 0.471, 'recall': 0.633, 'ret_recall': 0.944, 'oracle_percent': 0.114, 'oracle_rank': 6.049, 'prompt_length': 11070.615}
    },
    'TriviaQA': {
        'top_1': {'em': 0.592, 'f1': 0.679, 'prec': 0.658, 'recall': 0.767, 'ret_recall': 0.627, 'oracle_percent': 0.627, 'oracle_rank': 1.0, 'prompt_length': 236.653},
        "top_5": {'em': 0.645, 'f1': 0.727, 'prec': 0.706, 'recall': 0.812},
        "top_10": {'em': 0.671, 'f1': 0.752, 'prec': 0.729, 'recall': 0.837},
        "top_15": {'em': 0.673, 'f1': 0.757, 'prec': 0.733, 'recall': 0.843},
        'top_20': {'em': 0.677, 'f1': 0.758, 'prec': 0.735, 'recall': 0.84, 'ret_recall': 0.952, 'oracle_percent': 0.375, 'oracle_rank': 2.271, 'prompt_length': 2904.057},
        'top_40': {'em': 0.697, 'f1': 0.774, 'prec': 0.75, 'recall': 0.857, 'ret_recall': 0.975, 'oracle_percent': 0.316, 'oracle_rank': 2.883, 'prompt_length': 5714.368},
        'top_60': {'em': 0.7, 'f1': 0.775, 'prec': 0.752, 'recall': 0.856, 'ret_recall': 0.984, 'oracle_percent': 0.285, 'oracle_rank': 3.321, 'prompt_length': 8529.492},
        'top_80': {'em': 0.683, 'f1': 0.763, 'prec': 0.739, 'recall': 0.855, 'ret_recall': 0.986, 'oracle_percent': 0.263, 'oracle_rank': 3.459, 'prompt_length': 11346.743}
    },
    'hotpotQA': {
        'top_1': {'em': 0.234, 'f1': 0.355, 'prec': 0.361, 'recall': 0.408, 'ret_recall': 0.351, 'oracle_percent': 0.703, 'oracle_rank': 1.0, 'prompt_length': 195.152},
        "top_5": {'em': 0.2785, 'f1': 0.408, 'prec': 0.416, 'recall': 0.462},
        "top_10": {'em': 0.295, 'f1': 0.421, 'prec': 0.426, 'recall': 0.475},
        "top_15": {'em': 0.305, 'f1': 0.433, 'prec': 0.439, 'recall': 0.485},
        'top_20': {'em': 0.305, 'f1': 0.431, 'prec': 0.439, 'recall': 0.483, 'ret_recall': 0.688, 'oracle_percent': 0.069, 'oracle_rank': 3.172, 'prompt_length': 1800.811},
        'top_40': {'em': 0.323, 'f1': 0.453, 'prec': 0.461, 'recall': 0.506, 'ret_recall': 0.732, 'oracle_percent': 0.037, 'oracle_rank': 4.764, 'prompt_length': 3459.514},
        'top_60': {'em': 0.311, 'f1': 0.442, 'prec': 0.45, 'recall': 0.494, 'ret_recall': 0.751, 'oracle_percent': 0.025, 'oracle_rank': 5.917, 'prompt_length': 5106.305},
        'top_80': {'em': 0.305, 'f1': 0.441, 'prec': 0.449, 'recall': 0.495, 'ret_recall': 0.77, 'oracle_percent': 0.019, 'oracle_rank': 7.506, 'prompt_length': 6743.218}
    }
}



# todo: doc selection by pl: 500 1000 1500 2000 for llama (4k); 500 2000 4000 6000 8000 for gpt (16k)

qa_ret_doc_selection_pl_gpt_n_1 = {
    'NQ': {},
    'TriviaQA': {},
    'hotpotQA': {}
}

qa_ret_doc_selection_pl_llama_n_1 = {
    'NQ': {},
    'TriviaQA': {},
    'hotpotQA': {}
}

# todo: pl 1000 2000 4000 6000 8000 for code llama and gpt

code_ret_doc_selection_pl_llama_n_1 = {
    'conala': {

    },
    'DS1000': {

    },
    'pandas_numpy_eval': {

    }
}

code_ret_doc_selection_pl_gpt_n_1 = {
    'conala': {

    },
    'DS1000': {

    },
    'pandas_numpy_eval': {

    }
}



# todo: prompt length analysis: pl=500 1000 1500 2000 for llama; 500 2000 4000 6000 8000 for gpt

# qa_pl_analysis_gpt_n_1 = {
#     'NQ': {
#         'oracle_top10': {'em': 0.533, 'f1': 0.6916228634702651, 'prec': 0.6729883714002561, 'recall': 0.8258000000000005},
#         'distracting_top10': {'em': 0.0995, 'f1': 0.22505139697448193, 'prec': 0.23412652439254966, 'recall': 0.28269999999999995},
#         'random_top10': {'em': 0.1855, 'f1': 0.30140874647701077, 'prec': 0.30451733094639355, 'recall': 0.3886083333333334},
#         'irrelevant_diff_top10': {'em': 0.1505, 'f1': 0.2516590830240432, 'prec': 0.24821379062303991, 'recall': 0.34620833333333334},
#         'irrelevant_dummy_top10': {'em': 0.145, 'f1': 0.24138619528558955, 'prec': 0.2311666719250147, 'recall': 0.34219166666666667}
#     },
#     'TriviaQA': {
#         "oracle_top10": {'em': 0.7405, 'f1': 0.8167501274111777, 'prec': 0.7911091913482974, 'recall': 0.9011666666666666},
#         "distracting_top10": {'em': 0.403, 'f1': 0.4909749859018985, 'prec': 0.48126900979027726, 'recall': 0.5581903966131908},
#         "random_top10": {'em': 0.671, 'f1': 0.7316760810304767, 'prec': 0.7153646208437773, 'recall': 0.7961125},
#         "irrelevant_diff_top10": {'em': 0.67, 'f1': 0.7297007214197764, 'prec': 0.7142052979297553, 'recall': 0.7971416666666666},
#         "irrelevant_dummy_top10": {'em': 0.6745, 'f1': 0.739580804760392, 'prec': 0.7209649724559586, 'recall': 0.8175746212121213},
#     },
#     'hotpotQA': {
#         "oracle_top10": {'em': 0.5425, 'f1': 0.7073541396923203, 'prec': 0.7199568933992317, 'recall': 0.7624801587301592},
#         "distracting_top10": {'em': 0.1535, 'f1': 0.24765849641844792, 'prec': 0.2535140550948018, 'recall': 0.27620553058053043},
#         "random_top10": {'em': 0.206, 'f1': 0.290075118871929, 'prec': 0.2967565277125306, 'recall': 0.32681951936951953},
#         "irrelevant_diff_top10": {'em': 0.202, 'f1': 0.2963647082209858, 'prec': 0.3060181072716487, 'recall': 0.3290111832611831},
#         "irrelevant_dummy_top10": {'em': 0.2215, 'f1': 0.31129415865827115, 'prec': 0.32212524737409665, 'recall': 0.34101713564213576},
#     },
# }

# "oracle": {'pass@1': 0.38095238095238093},
#         "retrieved": {'pass@1': 0.21428571428571427},
#         "distracting": {'pass@1': 0.2261904761904762},
#         "random": {'pass@1': 0.2619047619047619},
#         "irrelevant_dummy": {'pass@1': 0.30952380952380953},
#         "irrelevant_diff": {'pass@1': 0.3333333333333333},
#         "none": {'pass@1': 0.21428571428571427}

code_pl_analysis_gpt_n_1 = {
    'conala': {
        'oracle': {
            'oracle': {'pass@1': 0.381, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.589, 'prompt_length': 820.869},
            'pl_500': {'pass@1': 0.333, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.226, 'prompt_length': 503.619},
            'pl_2000': {'pass@1': 0.357, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.552, 'prompt_length': 2010.036},
            'pl_4000': {'pass@1': 0.357, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.589, 'prompt_length': 3917.75},
            'pl_8000': {'pass@1': 0.321, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.589, 'prompt_length': 7127.048}
        },
        'random': {
            'random': {'pass@1': 0.262, 'ret_recall': 0.0, 'oracle_percent': 0.0, 'prompt_length': 402.667},
            'pl_500': {'pass@1': 0.321, 'ret_recall': 0.0, 'oracle_percent': 0.0, 'prompt_length': 491.19},
            'pl_2000': {'pass@1': 0.321, 'ret_recall': 0.0, 'oracle_percent': 0.0, 'prompt_length': 1997.595},
            'pl_4000': {'pass@1': 0.345, 'ret_recall': 0.0, 'oracle_percent': 0.0, 'prompt_length': 3990.845},
            'pl_8000': {'pass@1': 0.333, 'ret_recall': 0.0, 'oracle_percent': 0.0, 'prompt_length': 7983.607}
        },
        'irrelevant_diff': {
            'irrelevant_diff': {'pass@1': 0.333, 'prompt_length': 814.833},
            'pl_500': {'pass@1': 0.274, 'prompt_length': 511.476},
            'pl_2000': {'pass@1': 0.31, 'prompt_length': 2036.19},
            'pl_4000': {'pass@1': 0.333, 'prompt_length': 3966.048},
            'pl_8000': {'pass@1': 0.345, 'prompt_length': 7197.583}
        },
        'irrelevant_dummy': {
            'irrelevant_dummy': {'pass@1': 0.31, 'prompt_length': 814.321},
            'pl_500': {'pass@1': 0.274, 'prompt_length': 512.143},
            'pl_2000': {'pass@1': 0.345, 'prompt_length': 2037.429},
            'pl_4000': {'pass@1': 0.286, 'prompt_length': 3967.869},
            'pl_8000': {'pass@1': 0.321, 'prompt_length': 7195.548}
        }
    }
}

code_pl_analysis_llama_n_1 = {}


qa_pl_analysis_gpt_n_1 = {
    'NQ': {
        'oracle': {
            'oracle': {'em': 0.519, 'f1': 0.676, 'prec': 0.657, 'recall': 0.82, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.0, 'prompt_length': 228.052},
            'pl_500': {'em': 0.522, 'f1': 0.68, 'prec': 0.659, 'recall': 0.826, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.0, 'prompt_length': 488.079},
            'pl_2000': {'em': 0.532, 'f1': 0.689, 'prec': 0.67, 'recall': 0.825, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.0, 'prompt_length': 1981.419},
            'pl_4000': {'em': 0.535, 'f1': 0.692, 'prec': 0.674, 'recall': 0.828, 'ret_recall': 1.0, 'oracle_percent': 1.0, 'oracle_rank': 1.0, 'prompt_length': 3981.055}
        },
        'random': {
            'random': {'em': 0.164, 'f1': 0.265, 'prec': 0.263, 'recall': 0.355, 'ret_recall': 0.003, 'oracle_percent': 0.003, 'oracle_rank': 1.0, 'prompt_length': 230.421},
            'pl_500': {'em': 0.149, 'f1': 0.248, 'prec': 0.242, 'recall': 0.345, 'ret_recall': 0.011, 'oracle_percent': 0.004, 'oracle_rank': 1.619, 'prompt_length': 496.366},
            'pl_2000': {'em': 0.19, 'f1': 0.301, 'prec': 0.303, 'recall': 0.387, 'ret_recall': 0.035, 'oracle_percent': 0.004, 'oracle_rank': 6.507, 'prompt_length': 1976.963},
            'pl_4000': {'em': 0.182, 'f1': 0.292, 'prec': 0.293, 'recall': 0.379, 'ret_recall': 0.059, 'oracle_percent': 0.003, 'oracle_rank': 12.205, 'prompt_length': 3976.886}
        },
        'irrelevant_diff': {
            'irrelevant_diff': {'em': 0.142, 'f1': 0.2265, 'prec': 0.224, 'recall': 0.307, 'prompt_length': 228.579},
            'pl_500': {'em': 0.125, 'f1': 0.21, 'prec': 0.204, 'recall': 0.303, 'prompt_length': 489.673},
            'pl_2000': {'em': 0.161, 'f1': 0.265, 'prec': 0.261, 'recall': 0.362, 'prompt_length': 1987.57},
            'pl_4000': {'em': 0.156, 'f1': 0.258, 'prec': 0.257, 'recall': 0.35, 'prompt_length': 3999.585}
        },
        'irrelevant_dummy': {
            'irrelevant_dummy': {'em': 0.12, 'f1': 0.198, 'prec': 0.1875, 'recall': 0.288, 'prompt_length': 228.262},
            'pl_500': {'em': 0.111, 'f1': 0.197, 'prec': 0.186, 'recall': 0.296, 'prompt_length': 488.71},
            'pl_2000': {'em': 0.154, 'f1': 0.254, 'prec': 0.246, 'recall': 0.35, 'prompt_length': 1984.417},
            'pl_4000': {'em': 0.159, 'f1': 0.265, 'prec': 0.258, 'recall': 0.367, 'prompt_length': 3987.259}
        }
        # todo: need more exps, keep same semantic, add prompt lengths (need to pay attention to the position of the retrieved docs)
        # todo: is the random information itself can do this or the prompt length is the key? add retrieved, -> if retrieved also improve, then
        # todo: more prompt length can help LLM revoke its own knowledge, use pretend and ...
        # todo: get code results -> then discuss
    },
    'TriviaQA': {
        'oracle': {},
        'random': {},
        'irrelevant_diff': {},
        'irrelevant_dummy': {}
    },
    'hotpotQA': {
        'oracle': {},
        'random': {},
        'irrelevant_diff': {},
        'irrelevant_dummy': {}
    }
}

qa_pl_analysis_llama_n_1 = {
    'NQ': {
        'oracle': {
            'oracle': None,
            'pl_500': None,
            'pl_1000': None,
            'pl_2000': None
        },
        'random': {
            'oracle': None,
            'pl_500': None,
            'pl_1000': None,
            'pl_2000': None
        },
        'irrelevant_diff': {
            'oracle': None,
            'pl_500': None,
            'pl_1000': None,
            'pl_2000': None
        },
        'irrelevant_dummy': {
            'oracle': None,
            'pl_500': None,
            'pl_1000': None,
            'pl_2000': None
        }
    },
    'TriviaQA': {
        'oracle': {},
        'random': {},
        'irrelevant_diff': {},
        'irrelevant_dummy': {}
    },
    'hotpotQA': {
        'oracle': {},
        'random': {},
        'irrelevant_diff': {},
        'irrelevant_dummy': {}
    }
}
